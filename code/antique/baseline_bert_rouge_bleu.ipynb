{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import re\n",
    "import jsonlines\n",
    "import difflib\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import kendalltau\n",
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def truncate_and_tokenize(text, max_length=512):\n",
    "    # 使用BERT的分词器进行分词\n",
    "    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text, add_special_tokens=True)))\n",
    "\n",
    "    # 截断或填充以适应指定的最大长度\n",
    "    tokens = tokens[:max_length-2]  # 保留 [CLS] 和 [SEP] 标记\n",
    "\n",
    "    # 添加特殊标记 [CLS] 和 [SEP]\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "\n",
    "    # 将词汇转换为对应的ID\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # 创建PyTorch张量\n",
    "    input_ids_tensor = torch.tensor(input_ids).unsqueeze(0)\n",
    "\n",
    "    return input_ids_tensor\n",
    "\n",
    "def calculate_bert_score(sentence1, sentence2, model, tokenizer):\n",
    "    input_ids1 = truncate_and_tokenize(sentence1)\n",
    "    input_ids2 = truncate_and_tokenize(sentence2)\n",
    "\n",
    "    # 获取BERT模型的输出\n",
    "    with torch.no_grad():\n",
    "        outputs1 = model(input_ids1)\n",
    "        outputs2 = model(input_ids2)\n",
    "\n",
    "    # 获取最后一层的隐藏状态\n",
    "    last_hidden_states1 = outputs1.last_hidden_state\n",
    "    last_hidden_states2 = outputs2.last_hidden_state\n",
    "\n",
    "    # 取第一个位置（[CLS]标记）的隐藏状态作为句子表示\n",
    "    sentence_embedding1 = last_hidden_states1[:, 0, :]\n",
    "    sentence_embedding2 = last_hidden_states2[:, 0, :]\n",
    "\n",
    "    # 使用余弦相似度计算句子之间的相似度\n",
    "    similarity = F.cosine_similarity(sentence_embedding1, sentence_embedding2)\n",
    "\n",
    "    return similarity.item()\n",
    "\n",
    "model_name = \"../../models/bert-base-uncased\"\n",
    "\n",
    "from bert_score import score as bscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau\n",
    "import math\n",
    "\n",
    "def score(l1, l2, p=0.5):\n",
    "    \"\"\"\n",
    "    Calculates Ranked Biased Overlap (RBO) score.\n",
    "    l1 -- Ranked List 1\n",
    "    l2 -- Ranked List 2\n",
    "    \"\"\"\n",
    "    if l1 == None: l1 = []\n",
    "    if l2 == None: l2 = []\n",
    "\n",
    "    sl, ll = sorted([(len(l1), l1), (len(l2), l2)])\n",
    "    s, S = sl\n",
    "    l, L = ll\n",
    "    if s == 0: return 0\n",
    "\n",
    "    ss = set([])  # contains elements from the smaller list till depth i\n",
    "    ls = set([])  # contains elements from the longer list till depth i\n",
    "    x_d = {0: 0}\n",
    "    sum1 = 0.0\n",
    "    for i in range(l):\n",
    "        x = L[i]\n",
    "        y = S[i] if i < s else None\n",
    "        d = i + 1\n",
    "\n",
    "        if x == y:\n",
    "            x_d[d] = x_d[d - 1] + 1.0\n",
    "        else:\n",
    "            ls.add(x)\n",
    "            if y != None: ss.add(y)\n",
    "            x_d[d] = x_d[d - 1] + (1.0 if x in ss else 0.0) + (1.0 if y in ls else 0.0)\n",
    "        sum1 += x_d[d] / d * pow(p, d)\n",
    "\n",
    "    sum2 = 0.0\n",
    "    for i in range(l - s):\n",
    "        d = s + i + 1\n",
    "        sum2 += x_d[d] * (d - s) / (d * s) * pow(p, d)\n",
    "\n",
    "    sum3 = ((x_d[l] - x_d[s]) / l + x_d[s] / s) * pow(p, l)\n",
    "\n",
    "    rbo_ext = (1 - p) / p * (sum1 + sum2) + sum3\n",
    "    return rbo_ext\n",
    "\n",
    "def calculate_pearson_coefficient(seq1, seq2):\n",
    "    \"\"\"计算Pearson相关系数\"\"\"\n",
    "    if len(seq1) < 2 or len(seq2) < 2:\n",
    "        return 0\n",
    "    pearson_coefficient, _ = pearsonr(seq1, seq2)\n",
    "    return pearson_coefficient\n",
    "\n",
    "def calculate_spearman_coefficient(seq1, seq2):\n",
    "    \"\"\"计算Spearman相关系数\"\"\"\n",
    "    if len(seq1) < 2 or len(seq2) < 2:\n",
    "        return 0\n",
    "    spearman_coefficient, _ = spearmanr(seq1, seq2)\n",
    "    return spearman_coefficient\n",
    "\n",
    "def calculate_kendalltau_coefficient(seq1, seq2):\n",
    "    \"\"\"计算Kendall Tau相关系数\"\"\"\n",
    "    if len(seq1) < 2 or len(seq2) < 2:\n",
    "        return 0\n",
    "    kendall_coefficient, _ = kendalltau(seq1, seq2)\n",
    "    return kendall_coefficient\n",
    "\n",
    "def compute_rank_correlations(rank, rank_ours):\n",
    "    \"\"\"计算两个排名序列之间的相关系数\"\"\"\n",
    "    # 将排名转换为排名索引（1-based index）\n",
    "    rank_dict = {model: idx + 1 for idx, model in enumerate(rank)}\n",
    "    rank_ours_dict = {model: idx + 1 for idx, model in enumerate(rank_ours)}\n",
    "    \n",
    "    # 为每个排名生成一个序列\n",
    "    rank_sequence = [rank_dict.get(model, 0) for model in rank]\n",
    "    rank_ours_sequence = [rank_ours_dict.get(model, 0) for model in rank]\n",
    "\n",
    "    # 计算相关系数\n",
    "    kendall = calculate_kendalltau_coefficient(rank_sequence, rank_ours_sequence)\n",
    "    spearman = calculate_spearman_coefficient(rank_sequence, rank_ours_sequence)\n",
    "    pearson = calculate_pearson_coefficient(rank_sequence, rank_ours_sequence)\n",
    "\n",
    "    return kendall, spearman, pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_score(r):\n",
    "    with open('../../data/antique/ANTIQUE_S5/sample' + str(r) + '.json', 'r', encoding='utf-8-sig') as file:\n",
    "        datas = json.load(file)\n",
    "\n",
    "    # 初始化评分字典\n",
    "    bert_array = defaultdict(list)\n",
    "    bleu_array = defaultdict(list)\n",
    "    golden_array = defaultdict(list)\n",
    "    rouge1_array = defaultdict(list)\n",
    "    rouge2_array = defaultdict(list)\n",
    "    rougel_array = defaultdict(list)\n",
    "\n",
    "    rouge = Rouge()\n",
    "        \n",
    "    for data in tqdm(datas):\n",
    "        question = data['question']  # 获取问题\n",
    "        ground = data['answer'][0]  # 获取正确答案\n",
    "        doc_list = []\n",
    "        \n",
    "        for name, res in data['response'].items():\n",
    "            doc_list.append(res)\n",
    "        \n",
    "        # 计算BERT评分\n",
    "        bert_score_list = bscore(doc_list, [ground]*len(doc_list), model_type=model_name, device='cpu')[-1].tolist()\n",
    "        \n",
    "        for doc, bert_score in zip(doc_list, bert_score_list):\n",
    "            prediction = doc  # 因为res本身就是文本，所以可以直接使用\n",
    "            \n",
    "            # 计算ROUGE评分\n",
    "            scores = rouge.get_scores(prediction, ground)\n",
    "            rouge_l = scores[0][\"rouge-l\"][\"f\"]\n",
    "            rouge_1 = scores[0][\"rouge-1\"][\"f\"]\n",
    "            rouge_2 = scores[0][\"rouge-2\"][\"f\"]\n",
    "            \n",
    "            # 分词\n",
    "            predict_tokens = prediction.split()\n",
    "            ground_tokens = ground.split()\n",
    "            \n",
    "            # 计算BLEU评分\n",
    "            bleu_score = sentence_bleu([ground_tokens], predict_tokens)\n",
    "            \n",
    "            # 保存评分到字典\n",
    "            rouge1_array[question].append(rouge_1)\n",
    "            rouge2_array[question].append(rouge_2)\n",
    "            rougel_array[question].append(rouge_l)\n",
    "            bert_array[question].append(bert_score)\n",
    "            bleu_array[question].append(bleu_score)\n",
    "        golden_array[question] = data['rank']\n",
    "        \n",
    "    baseline_list = [bleu_array, rouge1_array, rouge2_array, rougel_array, bert_array]\n",
    "    for m in baseline_list:\n",
    "        kendall_scores = []\n",
    "        spearman_scores = []\n",
    "        pearson_scores = []\n",
    "        rbo_0_5_scores = [] \n",
    "        rbo_0_9_scores = []\n",
    "        for question, scores in m.items():\n",
    "            print(scores)\n",
    "            rank = golden_array[question]\n",
    "            \n",
    "            model_list = rank\n",
    "            rank_our = [label for label, s in sorted(zip(model_list, scores), key=lambda x: x[1], reverse=True)]\n",
    "            \n",
    "            print(rank_our)\n",
    "            rbo_0_5 = score(rank_our, rank, p=0.5)\n",
    "            rbo_0_9 = score(rank_our, rank, p=0.9)\n",
    "\n",
    "            # 计算每一对排名的相关系数\n",
    "            kendall, spearman, pearson = compute_rank_correlations(rank_our, rank)\n",
    "        \n",
    "            # 存储相关系数\n",
    "            kendall_scores.append(kendall)\n",
    "            spearman_scores.append(spearman)\n",
    "            pearson_scores.append(pearson)\n",
    "            rbo_0_5_scores.append(rbo_0_5)\n",
    "            rbo_0_9_scores.append(rbo_0_9)\n",
    "\n",
    "        avg_kendall = np.mean(kendall_scores)\n",
    "        avg_spearman = np.mean(spearman_scores)\n",
    "        avg_pearson = np.mean(pearson_scores)\n",
    "        avg_rbo_0_5 = np.mean(rbo_0_5_scores)\n",
    "        avg_rbo_0_9 = np.mean(rbo_0_9_scores)\n",
    "\n",
    "        # 输出平均结果\n",
    "        print(\"Kendall Tau: %.4f\" % avg_kendall)\n",
    "        print(\"Spearman: %.4f\" % avg_spearman)\n",
    "        print(\"Pearson: %.4f\" % avg_pearson)\n",
    "        print(f\"RBO (p=0.5): {avg_rbo_0_5:.4f}\")\n",
    "        print(f\"RBO (p=0.9): {avg_rbo_0_9:.4f}\")\n",
    "        \n",
    "        print('*' * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(1, 6):\n",
    "    count_score(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
