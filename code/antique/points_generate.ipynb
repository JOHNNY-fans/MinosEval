{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrying import retry\n",
    "import openai\n",
    "\n",
    "@retry(stop_max_attempt_number=5, wait_exponential_multiplier=1000, wait_exponential_max=10000)\n",
    "def generate(prompt, history=[]):\n",
    "\n",
    "    client = openai.OpenAI(api_key=\"sk-xxx\", base_url=\"xxx\")\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"xxx\",\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature = 0.\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''<<task>\n",
    "Based on the question, extract the main points from the reference answer. Each bullet point should directly reflect the specifics of the reference answer. </task>\n",
    "\n",
    "<formatting requirements>\n",
    "1. Please take the main points from your analysis and add them to a python list modeled after the format below, using as much concise language as possible. \n",
    "2. The output python list needs to be able to be loaded by json.loads(), the example is as follows:\n",
    "{\n",
    "\"key_points\": [\"point1\", \"point2\", \"point3\", ... , \"point n\"]\n",
    "}\n",
    "</formatting requirements>\n",
    "\n",
    "<note>\n",
    "1. analyze the shortcomings obtained and make sure that they are focused but do not need to be described or explained in detail, keep it concise and clear. \n",
    "2.Any output that is not formatted will cause the system to crash!\n",
    "</note>\n",
    "\n",
    "<example>\n",
    "## Input\n",
    "Question: what does the word remission mean when referring to cancer patients?\n",
    "Reference answer: Well say the doctor said to me that my mom was on her 6th year of remission it means that she has had no cancer cells in her body. And it also means thats how long the cancer has been gone for! Does that help? Good I hope it did!\n",
    "Output:\n",
    "{\n",
    "    \"key points\": [\"Remission refers to the absence of cancer cells in the body.\",\"It indicates how long the cancer has been gone.\",\"Remission can be measured in years or other timeframes.\"]\n",
    "}\n",
    "</example>\n",
    "\n",
    "## Then analyze the following answer and output a python list that matches the formatting\n",
    "Question: {question}\n",
    "Reference answer: {answer}\n",
    "Output:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def format_check(inputs, num=None):\n",
    "    if isinstance(inputs,dict) and set(['key_points']):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# 定义一个函数，接受一个 markdown 字符串作为参数\n",
    "def find_dicts(markdown):\n",
    "    # 定义一个空列表，用于存储找到的 dict\n",
    "    dicts = []\n",
    "    \n",
    "    if markdown == None:\n",
    "        return dicts\n",
    "\n",
    "    # 定义一个正则表达式，匹配 dict 的格式\n",
    "    pattern = r\"\\{[^{}]*\\}\"\n",
    "    # 使用 re.findall 方法，找出 markdown 字符串中所有匹配的子串\n",
    "    matches = re.findall(pattern, markdown)\n",
    "    # 遍历每个匹配的子串\n",
    "    for match in matches:\n",
    "        # 尝试将子串转换为 dict 类型，如果成功则添加到列表中\n",
    "        try:\n",
    "            d = eval(match)\n",
    "            if isinstance(d, dict):\n",
    "                dicts.append(d)\n",
    "        except:\n",
    "            # 如果转换失败，忽略该子串\n",
    "            pass\n",
    "    # 返回找到的 dict 列表\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import os\n",
    "import json\n",
    "\n",
    "import traceback\n",
    "def deal(item,file_path):\n",
    "    max_try_retry = 5\n",
    "    try_num = 0\n",
    "    content = prompt.replace('{question}',item['question']).replace('{answer}',item['answer'])\n",
    "    for _ in range(max_try_retry):\n",
    "        try:\n",
    "            response = generate(content)\n",
    "            tmps = find_dicts(response)[0]\n",
    "            \n",
    "            if format_check(tmps):\n",
    "                item['points'] = tmps['key_points']\n",
    "                with open(file_path, \"a+\", encoding=\"utf8\") as f:\n",
    "                    f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "                    f.flush()\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            str_e = str(e)\n",
    "            if try_num== max_try_retry:\n",
    "                break\n",
    "\n",
    "            if 'InvalidRequestError' in str_e:\n",
    "                if 'maximum context' in str_e:\n",
    "                    break\n",
    "                try_num += 1\n",
    "                continue\n",
    "            else:\n",
    "                traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(1, 6):\n",
    "    with open('../../data/antique/ANTIQUE_S5/sample' + str(r) + '.json', \"r\", encoding='utf-8-sig') as f:\n",
    "        L = json.load(f)\n",
    "        input_data = []\n",
    "        for i in L:\n",
    "            tmp = {}\n",
    "            tmp['id'] = i['id']\n",
    "            tmp['question'] = i['question']\n",
    "            tmp['label'] = i['label']\n",
    "            tmp['response'] = i['response']\n",
    "            tmp['rank'] = i['rank']\n",
    "            \n",
    "            # 当 label 为 1 时，将 answer 的不同部分分开处理\n",
    "            if tmp['label'] == 1:\n",
    "                tmp1 = tmp.copy()  # 复制当前字典\n",
    "                tmp1['answer'] = i['answer'][0]  # 赋值第一个 answer\n",
    "                input_data.append(tmp1)\n",
    "                \n",
    "                tmp2 = tmp.copy()  # 复制当前字典\n",
    "                tmp2['answer'] = i['answer'][1]  # 赋值第二个 answer\n",
    "                input_data.append(tmp2)\n",
    "    # 文件路径处理\n",
    "    file_path = \"../../data/antique/fact/sample\" + str(r) + \"_points.jsonl\"\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, 'w') as f:\n",
    "            pass\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        L = f.readlines()\n",
    "\n",
    "    if len(L) > 0:\n",
    "        finish_id = [json.loads(i)['id'] for i in L]\n",
    "    else:\n",
    "        finish_id = []\n",
    "\n",
    "    input_list = [i for i in input_data if i['id'] not in finish_id]\n",
    "\n",
    "    num_worker = 32\n",
    "\n",
    "    # 使用并行计算\n",
    "    with tqdm_joblib(desc=\"My calculation\", total=len(input_list)) as progress_bar:\n",
    "        Parallel(n_jobs=num_worker, prefer=\"threads\")([delayed(deal)(x, file_path=file_path) for x in input_list])\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        L = f.readlines()\n",
    "\n",
    "    new_LL = [json.loads(i) for i in L]\n",
    "\n",
    "    # 合并数据\n",
    "    merged_data = {}\n",
    "\n",
    "    for entry in new_LL:\n",
    "        id = entry[\"id\"]\n",
    "        points = entry[\"points\"]\n",
    "        answer = entry[\"answer\"]\n",
    "\n",
    "        if id in merged_data:\n",
    "            merged_data[id][\"points\"].append(points)\n",
    "            merged_data[id][\"answer\"].append(answer)\n",
    "        else:\n",
    "            merged_data[id] = {key: entry[key] for key in entry if key != \"points\" and key != \"answer\"}\n",
    "            merged_data[id][\"points\"] = [points]\n",
    "            merged_data[id][\"answer\"] = [answer]\n",
    "\n",
    "    with open(\"../../data/antique/fact/sample\" + str(r) + \"_points.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(list(merged_data.values()), f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
