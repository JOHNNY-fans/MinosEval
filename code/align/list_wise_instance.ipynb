{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "import time\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrying import retry\n",
    "import openai\n",
    "\n",
    "@retry(stop_max_attempt_number=5, wait_exponential_multiplier=1000, wait_exponential_max=10000)\n",
    "def generate(prompt, history=[]):\n",
    "\n",
    "    client = openai.OpenAI(api_key=\"sk-xxx\", base_url=\"xxx\")\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"xxx\",\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature = 0.\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/align/Align_Minos.json\",\"r\",encoding='utf-8-sig') as f:\n",
    "    L = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "for i in L:\n",
    "    tmp = {}\n",
    "    tmp['id'] = i['id']\n",
    "    tmp['question'] = i['question']\n",
    "    tmp['answer'] = i['answer']\n",
    "    tmp['response'] = i['response']\n",
    "    tmp['rank'] = i['rank']\n",
    "    tmp['label'] = i['label']\n",
    "    if tmp['label'] == 0:\n",
    "        input_data.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''请根据输入的开放式问题和参考答案,以及提供的不同层次的答案样例，根据回复的质量对输入的模型回复进行排序，顺序越靠前的质量越高。\n",
    "\n",
    "答案样例的层次有五个，分别是优秀、良好、中等、较差、极差。\n",
    "\n",
    "请遵循以下的示例 JSON 格式输出结果：\n",
    "{\n",
    "    \"rank\": [”模型x“,“xxx”,…]\n",
    "}\n",
    "\n",
    "问题：{question}\n",
    "\n",
    "参考答案：{ref}\n",
    "\n",
    "不同层次的答案示例：\n",
    "{instance}\n",
    "\n",
    "模型回复：\n",
    "{model_input}\n",
    "\n",
    "输出：\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def format_check(inputs, num):\n",
    "    if isinstance(inputs,dict) and set(list(inputs)) == set(['优秀','良好','中等','较差','极差']):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def format_check(inputs,num):\n",
    "    if isinstance(inputs,dict) and set(list(inputs)) == set(['rank']):\n",
    "        if len(inputs['rank']) == num:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# 定义一个函数，接受一个 markdown 字符串作为参数\n",
    "def find_dicts(markdown):\n",
    "    # 定义一个空列表，用于存储找到的 dict\n",
    "    dicts = []\n",
    "    \n",
    "    if markdown == None:\n",
    "        return dicts\n",
    "\n",
    "    # 定义一个正则表达式，匹配 dict 的格式\n",
    "    pattern = r\"\\{[^{}]*\\}\"\n",
    "    # 使用 re.findall 方法，找出 markdown 字符串中所有匹配的子串\n",
    "    matches = re.findall(pattern, markdown)\n",
    "    # 遍历每个匹配的子串\n",
    "    for match in matches:\n",
    "        # 尝试将子串转换为 dict 类型，如果成功则添加到列表中\n",
    "        try:\n",
    "            d = eval(match)\n",
    "            if isinstance(d, dict):\n",
    "                dicts.append(d)\n",
    "        except:\n",
    "            # 如果转换失败，忽略该子串\n",
    "            pass\n",
    "    # 返回找到的 dict 列表\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/align/non_fact/examples.jsonl\",'r',encoding='utf-8') as f:\n",
    "    L = f.readlines()\n",
    "id2instance = {}\n",
    "for i in L:\n",
    "    item = json.loads(i)\n",
    "    id2instance[item['id']] = json.dumps(item['example'],indent=4,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import os\n",
    "import json\n",
    "\n",
    "import traceback\n",
    "def deal(item,file_path):\n",
    "    max_try_retry = 5\n",
    "    try_num = 0\n",
    "    model2id = {}\n",
    "    id2model = {}\n",
    "    \n",
    "    # print(item['id'])\n",
    "    for num,model in enumerate(item['response']):\n",
    "        model2id[model] = '模型'+str(num+1)\n",
    "        id2model['模型'+str(num+1)] = model\n",
    "\n",
    "    converted_output = \"\\n\\n\".join(\n",
    "    [f\"{model2id[key]}: {value}\" for key, value in item['response'].items()]\n",
    ")\n",
    "    content = prompt.replace('{question}',item['question']).replace('{ref}',item['answer']).replace('{model_input}',converted_output).replace('{instance}',id2instance[item['id']])\n",
    "    for _ in range(max_try_retry):\n",
    "        try:\n",
    "            response = generate(content)\n",
    "            try:\n",
    "                tmps = json.loads(response)\n",
    "            except:\n",
    "                tmps = find_dicts(response)[0]\n",
    "            \n",
    "            if format_check(tmps,len(id2model)):\n",
    "                item['rank_our'] = [id2model[i] for i in tmps['rank']]\n",
    "                with open(file_path, \"a+\", encoding=\"utf8\") as f:\n",
    "                    f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "                    f.flush()\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            str_e = str(e)\n",
    "            if try_num== max_try_retry:\n",
    "                break\n",
    "\n",
    "            if 'InvalidRequestError' in str_e:\n",
    "                if 'maximum context' in str_e:\n",
    "                    break\n",
    "                try_num += 1\n",
    "                continue\n",
    "            else:\n",
    "                traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path =\"../../data/align/non_fact/listwise_instance.jsonl\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        pass\n",
    "\n",
    "with open(file_path,'r',encoding='utf-8') as f:\n",
    "    L = f.readlines()\n",
    "\n",
    "if len(L)>0:\n",
    "    finish_id = [json.loads(i)['id'] for i in L]\n",
    "else:\n",
    "    finish_id = []\n",
    "\n",
    "input_list = [i for i in input_data if i['id'] not in finish_id]\n",
    "\n",
    "from tqdm.contrib import tzip\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "\n",
    "num_worker = 16\n",
    "\n",
    "with tqdm_joblib(desc=\"My calculation\", total=len(input_list)) as progress_bar:\n",
    "    Parallel(n_jobs=num_worker,prefer=\"threads\")([delayed(deal)(x,file_path=file_path) for x in input_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path,'r',encoding='utf-8-sig') as f:\n",
    "    L = f.readlines()\n",
    "\n",
    "LL = []\n",
    "for i in L:\n",
    "    LL.append(json.loads(i))\n",
    "        \n",
    "datas = sorted(LL, key=lambda x: x[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"../../../models/mDeBERTa-v3-base-mnli-xnli\"\n",
    "with open(\"../../data/align/fact/align_points_\"+model_name.split('/')[-1]+\".json\", 'r', encoding='utf-8') as f:\n",
    "    datas1 = json.loads(f.read())\n",
    "    for data in datas1:\n",
    "        datas.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau\n",
    "import math\n",
    "\n",
    "def score(l1, l2, p=0.5):\n",
    "    \"\"\"\n",
    "    Calculates Ranked Biased Overlap (RBO) score.\n",
    "    l1 -- Ranked List 1\n",
    "    l2 -- Ranked List 2\n",
    "    \"\"\"\n",
    "    if l1 == None: l1 = []\n",
    "    if l2 == None: l2 = []\n",
    "\n",
    "    sl, ll = sorted([(len(l1), l1), (len(l2), l2)])\n",
    "    s, S = sl\n",
    "    l, L = ll\n",
    "    if s == 0: return 0\n",
    "\n",
    "    ss = set([])  # contains elements from the smaller list till depth i\n",
    "    ls = set([])  # contains elements from the longer list till depth i\n",
    "    x_d = {0: 0}\n",
    "    sum1 = 0.0\n",
    "    for i in range(l):\n",
    "        x = L[i]\n",
    "        y = S[i] if i < s else None\n",
    "        d = i + 1\n",
    "\n",
    "        if x == y:\n",
    "            x_d[d] = x_d[d - 1] + 1.0\n",
    "        else:\n",
    "            ls.add(x)\n",
    "            if y != None: ss.add(y)\n",
    "            x_d[d] = x_d[d - 1] + (1.0 if x in ss else 0.0) + (1.0 if y in ls else 0.0)\n",
    "        sum1 += x_d[d] / d * pow(p, d)\n",
    "\n",
    "    sum2 = 0.0\n",
    "    for i in range(l - s):\n",
    "        d = s + i + 1\n",
    "        sum2 += x_d[d] * (d - s) / (d * s) * pow(p, d)\n",
    "\n",
    "    sum3 = ((x_d[l] - x_d[s]) / l + x_d[s] / s) * pow(p, l)\n",
    "\n",
    "    rbo_ext = (1 - p) / p * (sum1 + sum2) + sum3\n",
    "    return rbo_ext\n",
    "\n",
    "def calculate_pearson_coefficient(seq1, seq2):\n",
    "    \"\"\"计算Pearson相关系数\"\"\"\n",
    "    if len(seq1) < 2 or len(seq2) < 2:\n",
    "        return 0\n",
    "    pearson_coefficient, _ = pearsonr(seq1, seq2)\n",
    "    return pearson_coefficient\n",
    "\n",
    "def calculate_spearman_coefficient(seq1, seq2):\n",
    "    \"\"\"计算Spearman相关系数\"\"\"\n",
    "    if len(seq1) < 2 or len(seq2) < 2:\n",
    "        return 0\n",
    "    spearman_coefficient, _ = spearmanr(seq1, seq2)\n",
    "    return spearman_coefficient\n",
    "\n",
    "def calculate_kendalltau_coefficient(seq1, seq2):\n",
    "    \"\"\"计算Kendall Tau相关系数\"\"\"\n",
    "    if len(seq1) < 2 or len(seq2) < 2:\n",
    "        return 0\n",
    "    kendall_coefficient, _ = kendalltau(seq1, seq2)\n",
    "    return kendall_coefficient\n",
    "\n",
    "def compute_rank_correlations(rank, rank_ours):\n",
    "    \"\"\"计算两个排名序列之间的相关系数\"\"\"\n",
    "    # 将排名转换为排名索引（1-based index）\n",
    "    rank_dict = {model: idx + 1 for idx, model in enumerate(rank)}\n",
    "    rank_ours_dict = {model: idx + 1 for idx, model in enumerate(rank_ours)}\n",
    "    \n",
    "    # 为每个排名生成一个序列\n",
    "    rank_sequence = [rank_dict.get(model, 0) for model in rank]\n",
    "    rank_ours_sequence = [rank_ours_dict.get(model, 0) for model in rank]\n",
    "\n",
    "    # 计算相关系数\n",
    "    kendall = calculate_kendalltau_coefficient(rank_sequence, rank_ours_sequence)\n",
    "    spearman = calculate_spearman_coefficient(rank_sequence, rank_ours_sequence)\n",
    "    pearson = calculate_pearson_coefficient(rank_sequence, rank_ours_sequence)\n",
    "\n",
    "    return kendall, spearman, pearson\n",
    "\n",
    "\n",
    "kendall_scores = []\n",
    "spearman_scores = []\n",
    "pearson_scores = []\n",
    "rbo_0_5_scores = [] \n",
    "rbo_0_9_scores = []\n",
    "\n",
    "print(len(datas))\n",
    "# 遍历每条数据，计算相关系数\n",
    "for data in datas:\n",
    "    rank = data['rank']\n",
    "    if \"points_score\" in data:\n",
    "        rank_our =  [key for key, value in sorted(data['points_score'].items(), key=lambda item: item[1], reverse=True)]\n",
    "    else:\n",
    "        rank_our = data['rank_our']\n",
    "\n",
    "    rbo_0_5 = score(rank_our, rank, p=0.5)\n",
    "    rbo_0_9 = score(rank_our, rank, p=0.9)\n",
    "\n",
    "    # 计算每一对排名的相关系数\n",
    "    kendall, spearman, pearson = compute_rank_correlations(rank_our, rank)\n",
    "    \n",
    "    # 存储相关系数\n",
    "    kendall_scores.append(kendall)\n",
    "    spearman_scores.append(spearman)\n",
    "    pearson_scores.append(pearson)\n",
    "    rbo_0_5_scores.append(rbo_0_5)\n",
    "    rbo_0_9_scores.append(rbo_0_9)\n",
    "    # print(kendall)\n",
    "\n",
    "# 计算平均值\n",
    "avg_kendall = np.mean(kendall_scores)\n",
    "avg_spearman = np.mean(spearman_scores)\n",
    "avg_pearson = np.mean(pearson_scores)\n",
    "avg_rbo_0_5 = np.mean(rbo_0_5_scores)\n",
    "avg_rbo_0_9 = np.mean(rbo_0_9_scores)\n",
    "\n",
    "# 输出平均结果\n",
    "print(\"Kendall Tau: %.4f\" % avg_kendall)\n",
    "print(\"Spearman: %.4f\" % avg_spearman)\n",
    "print(\"Pearson: %.4f\" % avg_pearson)\n",
    "print(f\"RBO (p=0.5): {avg_rbo_0_5:.4f}\")\n",
    "print(f\"RBO (p=0.9): {avg_rbo_0_9:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
